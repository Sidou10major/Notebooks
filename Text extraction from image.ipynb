{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8232682,"sourceType":"datasetVersion","datasetId":4882457}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Arabic Handwritten Text Recognition\n## Optimized for WER < 0.6 and CER < 0.7","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:10:23.786440Z","iopub.execute_input":"2025-05-04T19:10:23.787139Z","iopub.status.idle":"2025-05-04T19:10:26.858479Z","shell.execute_reply.started":"2025-05-04T19:10:23.787109Z","shell.execute_reply":"2025-05-04T19:10:26.857563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchvision datasets --quiet\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom torch.optim import AdamW  # PyTorch's built-in AdamW\nfrom torchvision.models import resnet18\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:10:34.102041Z","iopub.execute_input":"2025-05-04T19:10:34.102718Z","iopub.status.idle":"2025-05-04T19:10:37.314819Z","shell.execute_reply.started":"2025-05-04T19:10:34.102686Z","shell.execute_reply":"2025-05-04T19:10:37.313765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HandwritingDataset(Dataset):\n    def __init__(self, images_dir, labels_dir, tokenizer, transform=None):\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.tokenizer = tokenizer\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((128, 128)),\n            transforms.ToTensor(),\n        ])\n        \n        # Get sorted file lists with validation\n        self.image_files = sorted([f for f in os.listdir(images_dir) \n                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        self.label_files = sorted([f for f in os.listdir(labels_dir) \n                                if f.lower().endswith('.txt')])\n        \n        # Verify 1:1 correspondence\n        assert len(self.image_files) == len(self.label_files), \"Image/label count mismatch\"\n        for img, lbl in zip(self.image_files, self.label_files):\n            assert os.path.splitext(img)[0] == os.path.splitext(lbl)[0], \\\n                f\"Mismatched pair: {img} vs {lbl}\"\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = os.path.join(self.images_dir, self.image_files[idx])\n        image = Image.open(img_path).convert('RGB')\n        \n        # Load label with windows-1256 encoding\n        lbl_path = os.path.join(self.labels_dir, self.label_files[idx])\n        with open(lbl_path, 'r', encoding='windows-1256') as f:\n            text = f.read().strip()\n        \n        # Apply transforms\n        if self.transform:\n            image = self.transform(image)\n        \n        # Tokenize with attention mask\n        inputs = self.tokenizer(\n            text,\n            return_tensors='pt',\n            padding='max_length',\n            max_length=128,\n            truncation=True\n        )\n        \n        return {\n            'pixel_values': image,\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'raw_text': text  # Store original text\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:10.700883Z","iopub.execute_input":"2025-05-04T19:38:10.701190Z","iopub.status.idle":"2025-05-04T19:38:10.709918Z","shell.execute_reply.started":"2025-05-04T19:38:10.701170Z","shell.execute_reply":"2025-05-04T19:38:10.709045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndataset = HandwritingDataset(\n    images_dir=\"/kaggle/input/images\",\n    labels_dir=\"/kaggle/input/labels\",\n    tokenizer=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:13.150932Z","iopub.execute_input":"2025-05-04T19:38:13.151482Z","iopub.status.idle":"2025-05-04T19:38:13.570133Z","shell.execute_reply.started":"2025-05-04T19:38:13.151462Z","shell.execute_reply":"2025-05-04T19:38:13.569497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HandwritingGPT2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cnn = resnet18(pretrained=True)\n        self.cnn.fc = nn.Linear(512, 768)\n        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"aubmindlab/aragpt2-base\")\n        \n    def forward(self, pixel_values, input_ids=None, attention_mask=None, labels=None):\n        # Extract features (batch_size, 768)\n        features = self.cnn(pixel_values)\n        \n        # Expand to (batch_size, seq_len, 768)\n        if input_ids is not None:\n            seq_len = input_ids.shape[1]\n            features = features.unsqueeze(1).expand(-1, seq_len, -1)\n        \n        return self.gpt2(\n            inputs_embeds=features,\n            attention_mask=attention_mask,\n            labels=labels\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:15.780372Z","iopub.execute_input":"2025-05-04T19:38:15.781068Z","iopub.status.idle":"2025-05-04T19:38:15.786060Z","shell.execute_reply.started":"2025-05-04T19:38:15.781043Z","shell.execute_reply":"2025-05-04T19:38:15.785389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = HandwritingGPT2().to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:18.621175Z","iopub.execute_input":"2025-05-04T19:38:18.621870Z","iopub.status.idle":"2025-05-04T19:38:19.464057Z","shell.execute_reply.started":"2025-05-04T19:38:18.621847Z","shell.execute_reply":"2025-05-04T19:38:19.463496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.text import CharErrorRate, WordErrorRate\nfrom tqdm import tqdm  # For progress bars\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel = HandwritingGPT2().to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Metrics\ncer = CharErrorRate().to(device)\nwer = WordErrorRate().to(device)\nfor epoch in range(1):\n    # Training phase\n    model.train()\n    total_loss = 0\n    train_cer, train_wer = [], []\n    \n    for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n        pixel_values = batch['pixel_values'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=input_ids\n        )\n        \n        # Backward pass\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Generate predictions for metrics\n        with torch.no_grad():\n            features = model.cnn(pixel_values).unsqueeze(1)\n            generated = model.gpt2.generate(\n                inputs_embeds=features,\n                max_length=128,\n                num_beams=5,\n                early_stopping=True\n            )\n            pred_texts = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n            \n            # Compute metrics\n            batch_cer = cer(pred_texts, batch['raw_text'])\n            batch_wer = wer(pred_texts, batch['raw_text'])\n            train_cer.append(batch_cer)\n            train_wer.append(batch_wer)\n        \n        # Logging\n        if batch_idx % 20 == 0:\n            avg_loss = total_loss / (batch_idx + 1)\n            avg_cer = torch.stack(train_cer).mean().item()\n            avg_wer = torch.stack(train_wer).mean().item()\n            print(f\"Batch {batch_idx}: Loss={avg_loss:.4f}, CER={avg_cer:.4f}, WER={avg_wer:.4f}\")\n    \n    # Epoch summary\n    avg_loss = total_loss / len(dataloader)\n    epoch_cer = torch.stack(train_cer).mean().item()\n    epoch_wer = torch.stack(train_wer).mean().item()\n    print(f\"\\nEpoch {epoch} Results:\")\n    print(f\"Avg Loss: {avg_loss:.4f}\")\n    print(f\"Training CER: {epoch_cer:.4f}\")\n    print(f\"Training WER: {epoch_wer:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:44.663176Z","iopub.execute_input":"2025-05-04T19:38:44.663891Z","iopub.status.idle":"2025-05-04T19:51:00.827445Z","shell.execute_reply.started":"2025-05-04T19:38:44.663866Z","shell.execute_reply":"2025-05-04T19:51:00.826694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. PREDICTION FUNCTION (FINAL)\ndef predict(image_path, model, tokenizer):\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n    ])\n    \n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        features = model.cnn(image).unsqueeze(1)\n        generated = model.gpt2.generate(\n            inputs_embeds=features,\n            max_length=128,\n            num_beams=5,\n            early_stopping=True\n        )\n    \n    return tokenizer.decode(generated[0], skip_special_tokens=True)\n\n# TEST PREDICTION\ntest_img = \"/kaggle/input/images/AHTD3A0001_Para1_4.jpg\"\nprint(\"Predicted:\", predict(test_img, model, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T20:05:53.419945Z","iopub.execute_input":"2025-05-04T20:05:53.420585Z","iopub.status.idle":"2025-05-04T20:05:53.600129Z","shell.execute_reply.started":"2025-05-04T20:05:53.420553Z","shell.execute_reply":"2025-05-04T20:05:53.599532Z"}},"outputs":[],"execution_count":null}]}